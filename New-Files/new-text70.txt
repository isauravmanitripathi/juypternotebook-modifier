Validation by cross-validation


    In today's machine learning processes, cross-validation is essential to evaluating the accuracy of models. Machine learning models typically receive part of the total data available for training and are tested and evaluated using the remaining data. There is a possibility that the retained test data contain strong outliers from the model or can only be predicted poorly using the model due to other factors. It would normally be considered a bad model because it did not perform well on the test data set, regardless of how well it predicted the data in the training data set. Cross-validation can close this gap. The cross-validation test determines how well the model predicts data that was not used for training. As a result, selection bias or overfitting and underfitting can be detected.

    K-fold cross-validation is one of the most common cross-validation methods. Using the remaining k-1 sets, the training set is divided into k complementary sets and validated for each. It means that 8 models are trained and tested for k = 8. As a result of cross-validation, the average ability of the 8 models to predict unseen data is calculated. Most users perform K-fold cross-validation without knowing if it makes sense for the data set.

    We are working with S&P 500 time series data. Financial data, of course, is more interested in predicting future events than finding a model that explains past data. We are dealing with time series data, which makes most cross-validation methods difficult because they randomly select the amounts used for training and validation. As a result, it violates one of the basic assumptions of cross-validation, namely the assumption that the data are independent.

    Sklearn provides another method that does not violate data independence. K quantities are also included in the data set. It must be noted, however, that these quantities are not chosen randomly or overlapping, but contiguous based on the time series. Using the first observation set, training is carried out and tested and validated using the second observation set. Based on this, an error measure is created. As a next step, training is performed on the basis of the first two sets of observations, followed by testing and validation on the basis of the third, and a new error measure is calculated. After k iterations, an average error measure is calculated. Because we use all the data available for cross-validation, we can ensure that the data is independent so as not to violate the basic cross-validation assumptions.

    We define tscv as our TimeSeriesSplit method with the parameter n_splits = 5, which divides our data into 5 sets. However, Bergmeir et al. (2018) find that K-fold CV can be used for pure autoregressive models, but since our model is not purely autoregressive, we continue using TimeSeriesSplit. The K-fold-CV could be investigated for one of our models in the future.